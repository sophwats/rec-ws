{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"recommender\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, header, token_fun):\n",
    "    return sc.textFile(path).filter(lambda x: x!=header).map(lambda x: x.split(\",\")).map(token_fun).cache()\n",
    "\n",
    "links = load_data('ml-latest-small/links.csv', \n",
    "                  'movieId,imdbId,tmdbId', \n",
    "                  lambda tokens: (int(tokens[0]), int(tokens[1])))\n",
    "\n",
    "movies = load_data('ml-latest-small/movies.csv', \n",
    "                   'movieId,title,genres', \n",
    "                   lambda tokens: (int(tokens[0]),tokens[1]))\n",
    "\n",
    "ratings = load_data('ml-latest-small/ratings.csv', \n",
    "                    'userId,movieId,rating,timestamp', \n",
    "                    lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2])))\n",
    "\n",
    "tags = load_data('ml-latest-small/tags.csv', \n",
    "                    'userId,movieId,tag,timestamp', \n",
    "                    lambda tokens: (int(tokens[0]), int(tokens[1]), tokens[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(ratings, proportions):\n",
    "    split = ratings.randomSplit(proportions)\n",
    "    return {'training': split[0], 'validation': split[1], 'test': split[2]}\n",
    "    \n",
    "sets = split_sets(ratings, [0.63212056, 0.1839397, 0.1839397])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "rank = 10\n",
    "iterations = 10\n",
    "seed = 42\n",
    "model = ALS.train(sets['training'], rank, seed=seed, iterations=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = sets['validation'].map(lambda x: (x[0], x[1]))\n",
    "validation.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predictAll(validation)\n",
    "predictions.take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
